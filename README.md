# Stellar Cartography API

A high-performance spatial indexing API for EVE Frontier's stellar map data, capable of handling 100k+ star systems with sub-millisecond query times.

## Dependencies

This service depends on the [eve-frontier-tools](https://github.com/VULTUR-EveFrontier/eve-frontier-tools) data extraction pipeline to generate the required JSON files (`stellar_cartography.json` and `stellar_labels.json`) from EVE Frontier game files.

## Architecture

- **Spatial Index**: KD-tree (using `kiddo` crate) for O(log n) nearest neighbor queries
- **Web Framework**: Axum with HTTP/2 support + Tokio for async operations
- **Database**: SQLite for metadata storage with proper migrations
- **Data Pipeline**: Provided by [eve-frontier-tools](https://github.com/VULTUR-EveFrontier/eve-frontier-tools) - Node.js extraction from EVE Frontier game files

## Setup

### Prerequisites

1. **Install Rust**: <https://rustup.rs/>
2. **EVE Frontier Tools**: Clone and set up the [eve-frontier-tools](https://github.com/VULTUR-EveFrontier/eve-frontier-tools) repository
3. **Node.js**: For running the data extraction pipeline
4. **EVE Frontier**: Game must be installed with ResFiles available

### Data Extraction

The stellar cartography service requires JSON data files generated by the eve-frontier-tools pipeline:

```bash
# Clone the eve-frontier-tools repository
git clone https://github.com/VULTUR-EveFrontier/eve-frontier-tools.git

# Follow the eve-frontier-tools setup instructions to extract data
# This will generate the required stellar_cartography.json and stellar_labels.json files
```

Ensure the following files are available in the eve-frontier-tools data directory:

- `stellar_cartography.json` - Contains system, constellation, and region data
- `stellar_labels.json` - Contains localized names for systems, constellations, and regions

The stellar cartography service will automatically detect and load these files during startup.

### Building the API

```bash
# Build the project
cargo build --release

# Run the API server
cargo run --release
```

The API will start on `http://localhost:3000`

## API Endpoints

- `GET /health` - Health check
- `GET /systems/near?name={system_name}&radius={radius}` - Find systems within radius
- `GET /systems/nearest?name={system_name}&k={count}` - Find k-nearest systems
- `GET /systems/autocomplete?q={partial_name}` - Autocomplete system names

## Database Migrations

We use SQLx migrations for database schema management:

```bash
# Create a new migration
sqlx migrate add <migration_name>

# Run migrations (happens automatically on startup)
sqlx migrate run
```

## Performance

- **Startup**: Sub-second with binary cache, ~3-4 minutes for initial database seeding
- **Queries**: Sub-millisecond response times for spatial queries
- **Memory**: ~5MB binary cache, efficient KD-tree structure for 24k+ systems
- **Concurrency**: Fully async, handles thousands of concurrent requests
- **Data Integrity**: SHA-256 fingerprinting ensures cache validity

## Future Enhancements

- [x] Persistent spatial index serialization
- [x] System name resolution from localization files
- [ ] 3D visualization web UI
- [ ] Jump route calculations
- [ ] Regional statistics API
